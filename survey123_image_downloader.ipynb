{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ArcGIS Survey123 Attachment Downloader\n",
        "\n",
        "- Preview downloads to test naming\n",
        "- User-selected attributes (up to 4) for naming files in your preferred order\n",
        "- Automatic filename cleaning to remove long descriptive text before dates\n",
        "\n",
        "**Requirements:**\n",
        "- `arcgis` library (install via `conda install -c esri arcgis` or `!pip install arcgis`)\n",
        "- Valid ArcGIS Online/Enterprise credentials\n",
        "- Write access to `save_path`\n",
        "- To find the **survey_item_id**: \n",
        "  - Log in and open the survey123 project\n",
        "  - URL will have: https://survey123.arcgis.com/surveys/ **32 letters and numbers** /\n",
        "  - Copy the survey id and add to Section 2\n",
        "- Update the save path to align with the survey name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Installing Required Libraries\n",
        "\n",
        "Remove the hashtag below and run the cell to import the libraries.\n",
        "\n",
        "**If there is an error after running the setup and import section, try installing with conda instead:**\n",
        "```\n",
        "conda install -c esri arcgis\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install arcgis pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Import libraries and define connection variables. **Replace with your details.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import arcgis\n",
        "from arcgis.gis import GIS\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "from IPython.display import display\n",
        "\n",
        "# Configure logging for error tracking\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, \n",
        "    format='%(levelname)s: %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Performance Optimizations Setup\n",
        "\n",
        "Compile regex patterns once and define helper functions\n",
        "\n",
        "This section sets up the core optimizations:\n",
        "- Pre-compiled regex patterns (avoid recompiling for each file)\n",
        "- Reusable utility functions\n",
        "- OID lookup builder (key to fast downloads)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile regex patterns\n",
        "DATE_PATTERN = re.compile(r'(\\d{8}-\\d{4,6})')\n",
        "FILENAME_SANITIZER = re.compile(r'[^A-Za-z0-9_-]')\n",
        "LAYER_NAME_SANITIZER = re.compile(r'[^A-Za-z0-9]+')\n",
        "\n",
        "# ===== UTILITY FUNCTIONS =====\n",
        "\n",
        " # Single pass through columns\n",
        "def find_objectid_field(df: pd.DataFrame) -> str:\n",
        "    # First try exact matches (most common)\n",
        "    exact_matches = ['OBJECTID', 'FID', 'OID']\n",
        "    for col in df.columns:\n",
        "        if col.upper() in exact_matches:\n",
        "            return col\n",
        "    \n",
        "    # Then try partial matches\n",
        "    for col in df.columns:\n",
        "        if 'OBJECTID' in col.upper() or 'FID' in col.upper():\n",
        "            return col\n",
        "    \n",
        "    # Fallback to index\n",
        "    return df.index.name or 'index'\n",
        "\n",
        "# Reuse pre-compiled regex\n",
        "\n",
        "def sanitize_filename(text: str, max_length: int = 30) -> str:\n",
        "    safe_val = FILENAME_SANITIZER.sub('_', str(text))\n",
        "    return safe_val[:max_length]\n",
        "\n",
        "# Clean filename to just the name (no path) and remove descriptive text before date\n",
        "\n",
        "def clean_original_filename(filename: str) -> str:\n",
        "    \"\"\"Remove descriptive text before date in Survey123 filenames.\n",
        "    \n",
        "    Examples:\n",
        "        'image_of_plants_in_the_hoop-20250805-173647.jpg' → '20250805-173647.jpg'\n",
        "        'canopy_closure_photo-20260114-103045.jpg' → '20260114-103045.jpg'\n",
        "        'IMG_0234.jpg' → 'IMG_0234.jpg' (unchanged if no date pattern)\n",
        "    \"\"\"\n",
        "    # Use pre-compiled pattern\n",
        "    match = DATE_PATTERN.search(filename)\n",
        "    if match:\n",
        "        return filename[match.start():]\n",
        "    \n",
        "    # Fallback: split from right, take last part\n",
        "    parts = filename.rsplit('-', 2)\n",
        "    return parts[-1] if len(parts) > 1 else filename\n",
        "\n",
        "# Build OID lookup dictionary for O(1) access \n",
        "def build_oid_lookup(df: pd.DataFrame, objectid_field: str, \n",
        "                     naming_attributes: List[str]) -> Dict[int, Dict[str, Any]]:\n",
        "\n",
        "    oid_to_attrs = {}\n",
        "    unique_oids = df[objectid_field].unique()\n",
        "    \n",
        "    for oid in unique_oids:\n",
        "        row = df[df[objectid_field] == oid]\n",
        "        if len(row) > 0:\n",
        "            oid_to_attrs[oid] = {attr: row[attr].iloc[0] for attr in naming_attributes}\n",
        "    \n",
        "    return oid_to_attrs\n",
        "\n",
        "print(\"✓ Optimization functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Connect to ArcGIS\n",
        "\n",
        "Connecting to Survey123 and configuring the download location for photos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define variables\n",
        "portalURL = \"https://www.arcgis.com\"\n",
        "username = \"Please Add Your Username\"\n",
        "password = \"Please Add Your Password\"\n",
        "survey_item_id = \"Please Add Your Survey Item ID\"\n",
        "save_path = r\"C:\\temp\\(Please Add Folder Name)\"  # Use short path to avoid Windows 260-char limit\n",
        "keep_org_item = False\n",
        "\n",
        "# Create save directory\n",
        "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Connect to GIS\n",
        "try:\n",
        "    gis = GIS(portalURL, username, password, verify_cert=True)\n",
        "    print(f\"✓ Connected to survey: {gis.users.me.username}\")\n",
        "    \n",
        "    survey_by_id = gis.content.get(survey_item_id)\n",
        "    print(f\"✓ Survey: {survey_by_id.title}\")\n",
        "    \n",
        "    # Get related feature service\n",
        "    rel_fs = survey_by_id.related_items('Survey2Service','forward')[0]\n",
        "    print(f\"✓ Feature service: {rel_fs.title}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to connect to ArcGIS: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Export and View Data CSV\n",
        "\n",
        "Export survey data to Excel, then load main layer data for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Export data as Excel\n",
        "    print(\"Exporting survey data...\")\n",
        "    item_excel = rel_fs.export(title=survey_by_id.title + '_data', export_format='Excel')\n",
        "    excel_path = item_excel.download(save_path=save_path)[0]\n",
        "    print(f\"✓ Excel exported to: {excel_path}\")\n",
        "    \n",
        "    if not keep_org_item:\n",
        "        item_excel.delete()\n",
        "    \n",
        "    # Find main data layer (usually first layer with ObjectID)\n",
        "    main_layer = next((l for l in rel_fs.layers if hasattr(l, 'properties') and l.properties.name != 'Attachments'), None)\n",
        "    \n",
        "    if main_layer:\n",
        "        print(f\"\\n✓ Found main layer: {main_layer.properties.name}\")\n",
        "        data_df = main_layer.query(as_df=True)\n",
        "        print(f\"  Total records: {len(data_df)}\")\n",
        "        print(f\"  Total fields: {len(data_df.columns)}\")\n",
        "        \n",
        "        display(data_df.head())\n",
        "        print(\"\\nAvailable columns:\")\n",
        "        print(list(data_df.columns))\n",
        "        \n",
        "        # Find the ObjectID field using optimized function (OPTIMIZATION 1)\n",
        "        objectid_field = find_objectid_field(data_df)\n",
        "        print(f\"\\n✓ Using '{objectid_field}' as ObjectID field\")\n",
        "    else:\n",
        "        raise ValueError(\"Could not find main data layer\")\n",
        "        \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to export/load data: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Filename Cleaning Test\n",
        "\n",
        "Test the optimized filename cleaning function (uses pre-compiled regex).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Filename cleaning examples (using compiled regex patterns):\\n\")\n",
        "\n",
        "examples = [\n",
        "    \"image_of_plants_in_the_hoop-20250805-173647.jpg\",\n",
        "    \"plot32_180_Plant_stand_Distance_between_plants-20250704-1931.jpg\",\n",
        "    \"canopy_photo-20260114-103045.png\",\n",
        "    \"IMG_0234.jpg\"\n",
        "]\n",
        "\n",
        "for fname in examples:\n",
        "    cleaned = clean_original_filename(fname)\n",
        "    savings = len(fname) - len(cleaned)\n",
        "    print(f\"Original: {fname}\")\n",
        "    print(f\"Cleaned:  {cleaned}\")\n",
        "    print(f\"Saved:    {savings} characters\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Select Naming Attributes (with Order)\n",
        "\n",
        "Choose up to **4 attributes** to name photos in your preferred order.\n",
        "\n",
        "**Please select Object ID as the first attribute.**\n",
        "\n",
        "Use column names OR numbers from the list above.\n",
        "\n",
        "Format: `{attribute1}_{attribute2}_{attribute3}_{cleaned_filename}.jpg`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show numbered list of columns for easy selection\n",
        "print(\"\\nAvailable columns for naming:\")\n",
        "for i, col in enumerate(data_df.columns, 1):\n",
        "    print(f\" {i:2d}. {col}\")\n",
        "\n",
        "# Get user input with enhanced prompts\n",
        "naming_input = input(\n",
        "    f\"\\nEnter column name(s) for photo naming (comma-separated, or press Enter for '{objectid_field}'):\\n\"\n",
        "    \"Example: 'plot_id,treatment,date' or use numbers: '3,5,7'\\n>>> \"\n",
        ")\n",
        "\n",
        "# Parse input - split by comma and strip whitespace\n",
        "if naming_input.strip():\n",
        "    raw_inputs = [attr.strip() for attr in naming_input.split(',')]\n",
        "    naming_attributes = []\n",
        "    \n",
        "    # Check if user entered numbers or column names\n",
        "    for attr in raw_inputs:\n",
        "        if attr.isdigit():\n",
        "            # User entered column index number\n",
        "            idx = int(attr) - 1  # Convert to 0-based index\n",
        "            if 0 <= idx < len(data_df.columns):\n",
        "                naming_attributes.append(data_df.columns[idx])\n",
        "            else:\n",
        "                print(f\"Warning: Index {attr} out of range. Skipping.\")\n",
        "        else:\n",
        "            # User entered column name\n",
        "            naming_attributes.append(attr)\n",
        "else:\n",
        "    naming_attributes = [objectid_field]\n",
        "\n",
        "# Validate all attributes exist\n",
        "valid_attributes = []\n",
        "for attr in naming_attributes:\n",
        "    if attr in data_df.columns:\n",
        "        valid_attributes.append(attr)\n",
        "    else:\n",
        "        print(f\"Warning: '{attr}' not found in columns. Skipping.\")\n",
        "\n",
        "# Fallback to objectid_field if no valid attributes\n",
        "if not valid_attributes:\n",
        "    print(f\"No valid columns found. Using '{objectid_field}'.\")\n",
        "    naming_attributes = [objectid_field]\n",
        "else:\n",
        "    naming_attributes = valid_attributes\n",
        "\n",
        "# Limit to first 4 attributes to keep filenames reasonable\n",
        "if len(naming_attributes) > 4:\n",
        "    print(f\"\\nWarning: More than 4 attributes selected. Using first 4: {naming_attributes[:4]}\")\n",
        "    naming_attributes = naming_attributes[:4]\n",
        "\n",
        "print(f\"\\n✓ Using {naming_attributes} for naming photos (in this order).\")\n",
        "print(f\"Format: {('_'.join(['{' + attr + '}' for attr in naming_attributes]) + '_{cleaned_date}.jpg')}\")\n",
        "\n",
        "# Show example with actual data from first row\n",
        "if len(data_df) > 0:\n",
        "    sample_parts = []\n",
        "    for attr in naming_attributes:\n",
        "        sample_val = str(data_df[attr].iloc[0])[:20]  # First 20 chars\n",
        "        safe_val = sanitize_filename(sample_val)\n",
        "        sample_parts.append(safe_val)\n",
        "    \n",
        "    example_original = \"image_of_faba_beans_in_the_hoop-20250805-173647.jpg\"\n",
        "    example_cleaned = clean_original_filename(example_original)\n",
        "    \n",
        "    print(f\"\\nExample with your data:\")\n",
        "    print(f\" {'_'.join(sample_parts)}_{example_cleaned}\")\n",
        "    print(f\"\\nOriginal filename would have been:\")\n",
        "    print(f\" {'_'.join(sample_parts)}_{example_original}\")\n",
        "    print(f\" Savings: {len(example_original) - len(example_cleaned)} characters per file\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Preview: Download 2-3 Sample Images\n",
        "\n",
        "Test download first few images from layers with attachments with cleaned filenames.\n",
        "\n",
        "**Review the preview folder to verify naming before running full download.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    preview_path = Path(save_path) / 'preview'\n",
        "    preview_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    preview_count = 0\n",
        "    layers_with_att = [l for l in rel_fs.layers + rel_fs.tables if l.properties.hasAttachments == True]\n",
        "    \n",
        "    print(f\"Downloading preview images with naming: {' → '.join(naming_attributes)}\\n\")\n",
        "    \n",
        "    for layer in layers_with_att:\n",
        "        if preview_count >= 5:\n",
        "            break\n",
        "        \n",
        "        sanitized_name = LAYER_NAME_SANITIZER.sub('', layer.properties.name)\n",
        "        layer_folder = preview_path / f'{sanitized_name}_attachments'\n",
        "        layer_folder.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        oids = layer.query(where=\"1=1\", return_ids_only=True)['objectIds'][:2]  # 2 per layer max\n",
        "        \n",
        "        for oid in oids:\n",
        "            if preview_count >= 5:\n",
        "                break\n",
        "            \n",
        "            try:\n",
        "                atts = layer.attachments.get_list(oid=oid)\n",
        "                for att in atts[:1]:  # 1 per OID\n",
        "                    att_id = att['id']\n",
        "                    orig_path = layer.attachments.download(oid=oid, attachment_id=att_id, save_path=str(layer_folder))\n",
        "                    orig_name = Path(orig_path[0]).name\n",
        "                    \n",
        "                    # Clean original filename (remove descriptive text before date)\n",
        "                    clean_name = clean_original_filename(orig_name)\n",
        "                    \n",
        "                    # Build name from multiple attributes in specified order\n",
        "                    name_parts = []\n",
        "                    for attr in naming_attributes:\n",
        "                        # Find matching row by ObjectID\n",
        "                        matching_rows = data_df[data_df[objectid_field] == int(oid)]\n",
        "                        if len(matching_rows) > 0:\n",
        "                            att_val = matching_rows[attr].iloc[0]\n",
        "                        else:\n",
        "                            att_val = 'unknown'\n",
        "                        # Sanitize value for filename\n",
        "                        safe_val = sanitize_filename(str(att_val), max_length=20)\n",
        "                        name_parts.append(safe_val)\n",
        "                    \n",
        "                    # Combine attributes with cleaned filename\n",
        "                    new_name = f\"{'_'.join(name_parts)}_{clean_name}\"\n",
        "                    new_path = layer_folder / new_name\n",
        "                    Path(orig_path[0]).rename(new_path)\n",
        "                    preview_count += 1\n",
        "                    \n",
        "                    print(f\"Preview {preview_count}:\")\n",
        "                    print(f\" Original: {orig_name} ({len(orig_name)} chars)\")\n",
        "                    print(f\" New:      {new_name} ({len(new_name)} chars)\")\n",
        "                    print(f\" Saved:    {len(orig_name) - len(clean_name)} chars from filename cleaning\\n\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error downloading preview for OID {oid}: {str(e)}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"\\n✓ Preview complete. Check preview folder: {preview_path}\")\n",
        "    print(f\"If naming looks correct, proceed to Section 8 for full download.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Preview download failed: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Full Download\n",
        "\n",
        "Download ALL images with custom naming and cleaned filenames.\n",
        "\n",
        "**Run only after preview looks correct.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Build OID lookup dictionary ONCE before downloads\n",
        "    print(\"Building OID lookup dictionary...\")\n",
        "    oid_to_attrs = build_oid_lookup(data_df, objectid_field, naming_attributes)\n",
        "    print(f\"✓ Built lookup for {len(oid_to_attrs)} unique OIDs\\n\")\n",
        "    \n",
        "    # Full download with custom naming and filename cleaning\n",
        "    layers = rel_fs.layers + rel_fs.tables\n",
        "    total_downloaded = 0\n",
        "    total_chars_saved = 0\n",
        "    errors_encountered = 0\n",
        "    \n",
        "    for layer in layers:\n",
        "        if not layer.properties.hasAttachments:\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Processing layer: {layer.properties.name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        # Sanitize layer name for folder\n",
        "        sanitized_name = LAYER_NAME_SANITIZER.sub('', layer.properties.name)\n",
        "        layer_path = Path(save_path) / f\"{sanitized_name}_attachments\"\n",
        "        layer_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        csv_path = layer_path / f\"{layer.properties.name}_attachments.csv\"\n",
        "        csv_fields = ['Parent ObjectId'] + naming_attributes + ['Original Filename', 'New Filename', 'Attachment path']\n",
        "        \n",
        "        try:\n",
        "            with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                csvwriter = csv.writer(csvfile)\n",
        "                csvwriter.writerow(csv_fields)\n",
        "                \n",
        "                # Get all feature OIDs\n",
        "                query_result = layer.query(where=\"1=1\", return_ids_only=True, order_by_fields='objectid ASC')\n",
        "                feature_oids = query_result['objectIds']\n",
        "                total_features = len(feature_oids)\n",
        "                \n",
        "                print(f\"Total features: {total_features}\")\n",
        "                print(f\"Downloading attachments...\\n\")\n",
        "                \n",
        "                for idx, oid in enumerate(feature_oids, 1):\n",
        "                    try:\n",
        "                        current_oid_attachments = layer.attachments.get_list(oid=oid)\n",
        "                        \n",
        "                        if current_oid_attachments:\n",
        "                            # Use dictionary lookup (O(1)) instead of DataFrame filter (O(n))\n",
        "                            attr_vals = oid_to_attrs.get(oid, {attr: 'unknown' for attr in naming_attributes})\n",
        "                            attr_values = [attr_vals.get(attr, 'unknown') for attr in naming_attributes]\n",
        "                            name_parts = [sanitize_filename(attr_vals.get(attr, 'unknown')) for attr in naming_attributes]\n",
        "                            \n",
        "                            for att in current_oid_attachments:\n",
        "                                try:\n",
        "                                    attachment_id = att['id']\n",
        "                                    orig_path = layer.attachments.download(\n",
        "                                        oid=oid, \n",
        "                                        attachment_id=attachment_id, \n",
        "                                        save_path=str(layer_path)\n",
        "                                    )\n",
        "                                    orig_name = Path(orig_path[0]).name\n",
        "                                    \n",
        "                                    # Clean original filename\n",
        "                                    clean_name = clean_original_filename(orig_name)\n",
        "                                    chars_saved = len(orig_name) - len(clean_name)\n",
        "                                    total_chars_saved += chars_saved\n",
        "                                    \n",
        "                                    # Build new filename\n",
        "                                    new_name = f\"{'_'.join(name_parts)}_{clean_name}\"\n",
        "                                    new_path = layer_path / new_name\n",
        "                                    Path(orig_path[0]).rename(new_path)\n",
        "                                    \n",
        "                                    # Get relative path for CSV\n",
        "                                    att_relative_path = new_path.relative_to(Path(save_path))\n",
        "                                    csvwriter.writerow([oid] + attr_values + [orig_name, new_name, str(att_relative_path)])\n",
        "                                    \n",
        "                                    total_downloaded += 1\n",
        "                                    \n",
        "                                except Exception as e:\n",
        "                                    logger.error(f\"Failed to download attachment {att.get('id')} for OID {oid}: {str(e)}\")\n",
        "                                    errors_encountered += 1\n",
        "                                    continue\n",
        "                    \n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing OID {oid}: {str(e)}\")\n",
        "                        errors_encountered += 1\n",
        "                        continue\n",
        "                    \n",
        "                    # Progress reporting (~every 5%)\n",
        "                    progress_interval = max(1, total_features // 20)\n",
        "                    if idx % progress_interval == 0 or idx == total_features:\n",
        "                        percent = (idx / total_features) * 100\n",
        "                        print(f\"  [{idx:6d}/{total_features:6d}] {percent:5.1f}% complete...\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing layer {layer.properties.name}: {str(e)}\")\n",
        "            errors_encountered += 1\n",
        "            continue\n",
        "    \n",
        "    # Summary report\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ FULL DOWNLOAD COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total attachments downloaded: {total_downloaded:,}\")\n",
        "    print(f\"Total characters saved by filename cleaning: {total_chars_saved:,}\")\n",
        "    if total_downloaded > 0:\n",
        "        print(f\"Average savings per file: {total_chars_saved // total_downloaded} characters\")\n",
        "    print(f\"\\nExecution time: {elapsed_time:.1f} seconds\")\n",
        "    print(f\"Errors encountered: {errors_encountered}\")\n",
        "    print(f\"\\nFiles saved to: {save_path}\")\n",
        "    print(f\"Naming order: {' → '.join(naming_attributes)}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    # Display summary (only first 10 rows to save memory)\n",
        "    if total_downloaded > 0:\n",
        "        try:\n",
        "            summary_df = pd.read_csv(csv_path, nrows=10)\n",
        "            print(\"First 10 attachments:\")\n",
        "            display(summary_df)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not read CSV for summary: {str(e)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Download process failed: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Output locations:**\n",
        "- Excel data: `{save_path}/{survey_title}_data.xlsx`\n",
        "- Preview images: `{save_path}/preview/`\n",
        "- Full images: `{save_path}/LayerName_attachments/`\n",
        "- Mapping CSVs: Link object IDs to original and new filenames with all selected attributes\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "BootC",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
